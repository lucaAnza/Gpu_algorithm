\documentclass[12pt,a4paper]{report}
\usepackage[italian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{parskip}        % fa in modo che ogni paragrafo sia spaziato e non abbia l'indentazione
\usepackage{hyperref}       % Per gestire i collegamenti ipertestuali
\usepackage{listings} % Pacchetto per il codice
\usepackage{xcolor} % Opzionale, per colorare il codice
\lstset{
  language=C++,             % Linguaggio
  basicstyle=\ttfamily,      % Stile del testo
  keywordstyle=\color{blue}, % Colore delle parole chiave
  commentstyle=\color{gray}, % Colore dei commenti
  stringstyle=\color{red},   % Colore delle stringhe
  numbers=left,              % Numeri di riga a sinistra
  numberstyle=\tiny,         % Stile dei numeri di riga
  stepnumber=1,              % Mostra un numero di riga per ogni riga
  breaklines=true,           % Spezza automaticamente le righe lunghe
  frame=single,              % Cornice attorno al codice
  tabsize=4                  % Imposta la dimensione del tab
}

\usepackage{amsmath,amssymb,amsthm}
\usepackage{fancyhdr}
\usepackage{indentfirst}
\usepackage{microtype}
\usepackage{mathrsfs}
\usepackage{setspace}
\usepackage{graphicx}  % Serve per le immagini
\usepackage{float}
\usepackage{subfigure}
\usepackage{subcaption}

\graphicspath{ {./images/} }
\newcommand{\sign}{\text{sign}}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    urlcolor=blue,
    pdftitle={La metodologia di apprendimento automatico SVM per la regressione: applicazione al problema del testo sfocato},
    pdfpagemode=FullScreen,
    }

\newcommand{\omgv}{\overrightarrow{\omega}}
\newcommand{\xv}{\overrightarrow{x}}
\newcommand{\uv}{\overrightarrow{u}}
\newcommand{\xvo}{\overrightarrow{x_O}}
\newcommand{\xvx}{\overrightarrow{x_X}}
\newcommand{\xvi}{\overrightarrow{x_i}}
\newcommand{\xvj}{\overrightarrow{x_j}}

\begin{document}

\begin{titlepage}
\begin{center}
{{\Large{\textsc{Università degli Studi \\ \vspace{2mm} di Modena e Reggio Emilia}}}} \rule[0.1cm]{14cm}{0.1mm}
\rule[0.5cm]{14cm}{0.6mm}
{\small{\bf DIPARTIMENTO DI SCIENZE FISICHE, INFORMATICHE E MATEMATICHE\\
Corso di Laurea in Informatica}}

\end{center}
\vspace{20mm}
\begin{center}
{\LARGE{\bf Accelerazione di algoritmi di Stereo Matching su GPU per sistemi SLAM  }}\\
\vspace{3mm}
\end{center}
\vspace{40mm}
\par
\noindent
\begin{minipage}[t]{0.47\textwidth}
{\large{\bf Relatore:\\Dott. Filippo Muzzin}} \\
{\large{\bf Corelatore:\\Prof. Nicola Capodieci}}
\end{minipage}
\hfill
\begin{minipage}[t]{0.47\textwidth}\raggedleft
{\large{\bf Tesi di Laurea di:\\
Luca Anzaldi}}
\end{minipage}
\vspace{20mm}
\begin{center}
{\large{\bf Anno Accademico 2023/2024}}
\end{center}
\end{titlepage}

\tableofcontents

\chapter{Introduzione}

L’evoluzione delle tecnologie informatiche ha portato a una crescente necessità di eseguire calcoli complessi in tempi ridotti, specialmente nei settori che richiedono grandi potenze computazionali come la grafica, l'intelligenza artificiale e la simulazione scientifica. Per far fronte a queste esigenze, si sono sviluppati linguaggi e framework specifici per la programmazione parallela, che permettono di sfruttare le capacità di elaborazione simultanea di più core o unità di calcolo.

Questa tesi si propone di analizzare e implementare un percorso di ottimizzazione per un algoritmo di \textbf{Stereo Matching} utilizzando la tecnologia \textbf{CUDA} (Compute Unified Device Architecture), sviluppata da NVIDIA per l'elaborazione parallela su GPU. Il problema del Stereo Matching è una delle questioni fondamentali nel campo della visione artificiale e consiste nel calcolare la disparità tra due immagini catturate da punti di vista leggermente diversi (come accade negli occhi umani) al fine di ricostruire la profondità e ottenere una rappresentazione tridimensionale della scena.

Gli algoritmi di \textbf{Stereo Matching} tipicamente richiedono un'intensa capacità di calcolo, soprattutto quando si cerca di ottenere risultati accurati su immagini ad alta risoluzione. L’obiettivo principale di questa tesi è ottimizzare l'esecuzione di uno di questi algoritmi sfruttando le caratteristiche dell'elaborazione parallela offerta dalle GPU tramite CUDA. La scelta di utilizzare \textbf{CUDA} è motivata dalla sua capacità di distribuire il carico computazionale su un numero elevato di core, rendendo possibile l’elaborazione simultanea di grandi quantità di dati, e migliorando significativamente le prestazioni rispetto all'implementazione su \textbf{CPU tradizionali}.

Nella trattazione verrà esaminato l'approccio usato per ottimizare l'algoritmo  , con un'attenzione particolare all'efficienza e alla precisione, e si approfondiranno i principi di ottimizzazione parallela, analizzando tecniche come la decomposizione del problema in thread e la gestione ottimale della memoria della \textbf{GPU}. La tesi illustrerà anche i vantaggi e le sfide nell'implementare un algoritmo di Stereo Matching su architettura parallela, discutendo sia gli aspetti teorici che quelli pratici, con test sperimentali che metteranno a confronto le performance dell'algoritmo ottimizzato rispetto a versioni non parallele o meno efficienti.



\chapter{Panoramica dei linguaggi paralleli e specifiche di CUDA}

\section{Introduzione}

La programmazione parallela si basa sull’esecuzione contemporanea di più istruzioni o blocchi di codice, sfruttando risorse hardware multiprocessore, multicore o acceleratori come le GPU (\textbf{Graphics Processing Units}). A differenza della programmazione sequenziale tradizionale, che esegue le istruzioni una dopo l’altra, la programmazione parallela permette di dividere i problemi in sottoproblemi più piccoli, i quali possono essere risolti simultaneamente, aumentando l’efficienza complessiva del sistema.

Uno dei linguaggi di programmazione parallela più diffusi è \textbf{CUDA} (Compute Unified Device Architecture), un framework sviluppato da \textit{NVIDIA}.    Esso consente di sfruttare la potenza delle GPU per eseguire calcoli ad alte prestazioni, trasformando le unità grafiche in potenti strumenti di elaborazione general-purpose. Attraverso CUDA, gli sviluppatori possono scrivere codice in \textbf{C}, \textbf{C++} o Python, che viene successivamente parallelizzato e distribuito tra le varie unità di calcolo della GPU.

Inoltre, esistono altri linguaggi e API orientati alla programmazione parallela, come OpenCL, che offre un approccio più generalista e multi-piattaforma, permettendo di sfruttare anche altre tipologie di hardware, come \textbf{CPU} e \textbf{FPGA}. Questi strumenti sono oggi fondamentali per affrontare i carichi di lavoro che richiedono un’enorme quantità di calcoli in tempi contenuti, portando significativi benefici in campi come il machine learning, la crittografia, l’elaborazione video e le simulazioni fisiche.

\section{Differenza di architettura tra CPU e GPU}

L'architettura della CPU e della GPU può essere confrontata in maniera sintetica. La CPU è progettata per ridurre la latenza, ossia cerca di ottenere i risultati delle operazioni nel minor tempo possibile. Per fare questo, dispone di \textbf{cache L1} di grandi dimensioni, che aiutano a ridurre la latenza media dei dati, e utilizza poche unità logiche aritmetiche ad alte prestazioni per calcolare velocemente i risultati. I modelli moderni di CPU sfruttano anche il parallelismo a livello di istruzione, elaborando in anticipo risultati parziali per ridurre ulteriormente i tempi di attesa. Al contrario, l'architettura della GPU è orientata al \textbf{throughput}, ovvero al volume di operazioni elaborate simultaneamente. Poiché contiene un numero elevato di processori paralleli, non può dotarli di cache L1 grandi come quelle delle CPU. Di conseguenza, gli accessi alla memoria sono più lenti, causando maggiori latenze. Tuttavia, quando la GPU esegue molti più thread rispetto ai suoi core fisici (situazione chiamata “\textit{over-subscription}”), riesce a nascondere queste latenze passando rapidamente l'esecuzione da un thread all'altro.


\begin{figure}[h]
    \includegraphics[width=1\linewidth]{img/CPUvsGPU.png}
    \caption{Differenza di architettura \cite{CUDAtutorial}}
\end{figure}

I \textbf{thread} della GPU sono molto più leggeri rispetto a quelli della CPU, il che rende più efficiente il loro passaggio da uno all'altro. Anche se le latenze possono essere più elevate, la capacità di commutare rapidamente i thread e di gestire più istruzioni in parallelo permette alla GPU di mantenere un \textbf{throughput elevato} durante l'elaborazione. Pertanto, i vantaggi di utilizzare le GPU per calcoli intensivi aumentano all'aumentare del numero di thread impiegati per un determinato compito.

\section{Il modello di esecuzione di CUDA}

Nel modello di esecuzione di CUDA esistono due tipi di funzioni principali:

\begin{itemize}
    \item \texttt{\_\_global\_\_}: Queste funzioni sono chiamate \textit{kernel functions} e possono essere invocate dalla \textit{host} (CPU) per essere eseguite sulla \textit{device} (GPU). Sono definite con il qualificatore \texttt{\_\_global\_\_} e devono essere invocate con una sintassi speciale, specificando il numero di thread e blocchi da lanciare. Una caratteristica particolare delle funzioni \texttt{\_\_global\_\_} è che il loro tipo di ritorno deve essere sempre \texttt{void}. Ecco un esempio di dichiarazione di una funzione \texttt{\_\_global\_\_}:

    \begin{lstlisting}
    __global__ void myKernel(int *data) {
        // Codice da eseguire sulla GPU
    }
    \end{lstlisting}

    \item \texttt{\_\_device\_\_}: Queste funzioni possono essere chiamate solo da altre funzioni eseguite sulla \textit{device} (ovvero dalla GPU stessa) e non possono essere invocate direttamente dalla \textit{host}. Le funzioni \texttt{\_\_device\_\_} sono eseguite sulla GPU e possono avere qualsiasi tipo di ritorno. Ecco un esempio di dichiarazione di una funzione \texttt{\_\_device\_\_}:

    \begin{lstlisting}
    __device__ int square(int x) {
        return x * x;
    }
    \end{lstlisting}
\end{itemize}

\begin{figure}[h]
    \includegraphics[width=1\linewidth]{img/basic_kernel.png}
    \caption{Kernel di base (1 blocco, 12 thread) \cite{CUDAtutorial}}
\end{figure}

\section{Sincronizzazione}

Di base, CUDA presume che, se una funzione può dipendere da un'altra, tale dipendenza sarà garantita. Pertanto, CUDA gestisce la sincronizzazione tra le diverse funzioni per assicurare che vengano eseguite nell'ordine corretto.
Ecco alcuni esempi:

\begin{itemize}
    \item Il kernel B è successivo al kernel A. Allora il kernel B aspetterà che il primo kernel abbia finito l'esecuzione.
    \item Se stiamo scrivendo i dati sulla GPU il compilatore aspetterà a lanciare la funzione di tipo \texttt{\_\_kernel\_\_}.
    \item Se vogliamo scrivere i dati dalla GPU \(\rightarrow\) CPU il compilatore aspetterà che la funzione di tipo \texttt{\_\_kernel\_\_} sia terminata.
\end{itemize}

\vspace{8mm}
\hrule
\vspace{3mm}

È possibile ovviamente sincronizzare manualmente l'esecuzione in un programma CUDA, in modo tale da controllare l'ordine di esecuzione tra kernel, thread o tra operazioni tra CPU e GPU.  I principali metodi di sincronizzazione sono i seguenti:

\begin{itemize}
    \item \textbf{Sincronizzazione tra Host e Device:}
    \begin{itemize}
        \item \texttt{cudaDeviceSynchronize()}: Questa funzione viene chiamata dalla CPU per aspettare che tutte le operazioni precedenti lanciate sulla GPU siano completate. Viene utilizzata per assicurarsi che il kernel abbia terminato l'esecuzione prima di continuare con altre operazioni sul codice host.
    \end{itemize}
    
    \item \textbf{Sincronizzazione tra Thread di un Kernel:}
    \begin{itemize}
        \item \texttt{\_\_syncthreads()}: Questa funzione viene chiamata all'interno di un kernel e sincronizza tutti i thread all'interno di un blocco. Ogni thread deve raggiungere il punto in cui è presente \texttt{\_\_syncthreads()} prima di poter proseguire, ed è utile quando c'è bisogno che tutti i thread completino un’operazione comune prima di procedere ulteriormente. Va notato che \texttt{\_\_syncthreads()} funziona solo all'interno di un singolo blocco di thread, non tra diversi blocchi.
    \end{itemize}
    
    \item \textbf{Stream e Eventi per Sincronizzazione Personalizzata:}
    \begin{itemize}
        \item CUDA supporta l'uso di stream e eventi per una sincronizzazione più avanzata. Gli stream permettono di lanciare kernel in modo asincrono e indipendente tra loro, mentre gli eventi vengono utilizzati per sincronizzare l'esecuzione tra diversi stream o per misurare il tempo di esecuzione tra operazioni.
    \end{itemize}
\end{itemize}

\section{Gerarchia di esecuzione di CUDA}

La gerarchia di esecuzione in CUDA si basa su un'architettura parallela organizzata in livelli, che consente di suddividere il lavoro complessivo in unità più piccole. Il codice viene eseguito in parallelo su molte unità di calcolo, con la possibilità di gestire grandi quantità di operazioni simultaneamente. Questa struttura permette di scalare l'esecuzione in base alle esigenze, ottimizzando l'uso delle risorse disponibili per migliorare le prestazioni complessive.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{img/hierarchy.png}
    \caption{Illustrazione della gerarchia di esecuzione di CUDA \cite{CUDAhierarchy}}
\end{figure}

\subsection*{Thread}
Il \textit{thread} è l'unità di base di esecuzione in CUDA. Ogni thread esegue un'istanza del codice definito in un \textit{kernel}. I thread possono essere identificati tramite un identificatore univoco (\texttt{threadIdx}) che li distingue dagli altri all'interno del loro \textit{block}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{img/cuda_thread.png}
    \caption{Thread nel modello di CUDA \cite{CUDA_blog}}
\end{figure}

\subsection*{Block}
I thread sono organizzati in \textit{block}, che rappresentano un gruppo di thread che può essere eseguito insieme sulla GPU. Ogni block è indipendente dagli altri, e ogni thread all'interno di un block è identificato da un indice (\texttt{threadIdx}). La dimensione massima di un block dipende dall'architettura della GPU, ma tipicamente contiene fino a 1024 thread. L'indice del block all'interno della griglia è fornito da \texttt{blockIdx}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{img/cuda_block.png}
    \caption{Blocco(block) nel modello di CUDA \cite{CUDA_blog}}
\end{figure}



\subsection*{Grid}
I block sono organizzati in una \textit{grid}. Una grid è una collezione di block, e la dimensione della grid è determinata al momento del lancio del kernel. L'indice del block nella grid è indicato da \texttt{blockIdx}, che permette ai block di essere distinti e indirizzati individualmente.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{img/cuda_grid.png}
    \caption{Griglia(grid) nel modello di CUDA \cite{CUDA_blog}}
\end{figure}



\subsection*{Warp}
Un concetto fondamentale nella gerarchia di esecuzione di CUDA è il \textit{warp}. Un warp è un insieme di \textbf{32 thread} che vengono eseguiti in parallelo all'interno di un singolo block. La GPU esegue i thread in gruppi di warp, e tutti i thread di un warp seguono lo stesso percorso di esecuzione (flusso di controllo), il che significa che i thread all'interno di un warp sono soggetti a esecuzione sincrona. Se i thread di un warp prendono percorsi diversi (ad esempio in caso di branch divergenti), la GPU eseguirà i percorsi separatamente, riducendo l'efficienza.



\section{Tipi di memoria}

Nell'architettura CUDA, esistono diversi tipi di memoria, ognuno con caratteristiche specifiche in termini di accesso, latenza e visibilità.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{img/type_of_memory.png}
    \caption{Tipi di memoria \cite{CUDAtutorial}}
\end{figure}


\subsection{Memoria condivisa (Shared Memory)}
La \textit{memoria condivisa} è una memoria veloce e a bassa latenza, accessibile da tutti i thread all'interno dello stesso blocco. È utile per condividere dati tra thread e ridurre l'accesso alla memoria globale, ottimizzando le prestazioni dei programmi paralleli. La memoria condivisa è limitata in quantità e deve essere gestita con attenzione.

\subsection{Memoria globale (Global Memory)}
La \textit{memoria globale} è la memoria principale accessibile da tutti i thread e blocchi. Essa ha una latenza elevata, quindi è preferibile minimizzarne l'utilizzo, soprattutto se si può sfruttare la memoria condivisa. Tuttavia, la memoria globale ha una capacità molto maggiore rispetto alla memoria condivisa.

\subsection{Memoria locale (Local Memory)}
La \textit{memoria locale} è assegnata ai singoli thread e viene utilizzata per memorizzare variabili private. Nonostante il nome, questa memoria è fisicamente una parte della memoria globale, e pertanto ha tempi di accesso relativamente lenti.

\subsection{Memoria costante (Constant Memory)}
La \textit{memoria costante} è una memoria a sola lettura che viene utilizzata per valori che rimangono invariati durante l'esecuzione di un kernel. Essendo memorizzata nella cache, ha una latenza più bassa rispetto alla memoria globale quando viene letta simultaneamente da più thread.

\subsection{Memoria di texture (Texture Memory)}
La \textit{memoria di texture} è progettata per il recupero efficiente di dati strutturati e può essere utilizzata per migliorare le prestazioni in applicazioni che richiedono accessi irregolari ai dati. Viene spesso usata in ambiti grafici, ma è accessibile anche per scopi generali.

\subsection{Registri}
I \textit{registri} sono la forma di memoria più veloce in CUDA e sono utilizzati per memorizzare variabili temporanee all'interno dei thread. Ogni thread ha accesso ai propri registri, che sono molto limitati in numero. Un uso inefficiente dei registri può portare al cosiddetto \textit{spill} nella memoria locale, con conseguente rallentamento delle prestazioni.







\chapter{Panoramica della localizzazione e mappatura simultanea }

\section{Python}

È un linguaggio di programmazione ad alto livello, interpretato e

\chapter{Esperimenti e scopo della ricerca}

\section{Introduzione}

L'obiettivo di questo capitolo è ottimizzare un algoritmo di \textbf{Stereo Matching} attraverso la parallelizzazione. In particolare, l'algoritmo contiene già una fase di estrazione \textit{ORB (Oriented FAST and Rotated BRIEF)} parallelizzata, ma vi sono ancora altre parti del processo che possono essere migliorate sfruttando la parallelizzazione.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{img/start_point_opt.png}
    \caption{Stato iniziale del programma \cite{ORB_SLAM_2}}
\end{figure}


Per raggiungere questo obiettivo, il linguaggio di programmazione CUDA verrà utilizzato per implementare soluzioni che consentano di sfruttare le capacità di calcolo parallelo delle GPU, al fine di ridurre il tempo di elaborazione dell'algoritmo e migliorare le prestazioni complessive.
Nelle seguenti sezioni verrano mostrati i passaggi per la realizzazione del progetto.


\section{Installazione del sistema operativo}

Il sistema operativo scelto per il progetto è \textbf{Linux Mint 21.3}, è una distribuzione basata su \textit{Ubuntu} che offre un ambiente desktop intuitivo e facile da usare. Tra i vantaggi principali di Linux Mint e di Linux in generale troviamo la stabilità, la flessibilità, e una comunità di supporto attiva. \textit{Linux Mint}, in particolare, offre un'interfaccia utente simile a \textit{Windows}, rendendo la transizione semplice per gli utenti che non hanno familiarità con altri sistemi Linux.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.2\linewidth]{img/mint_logo.png}
    \caption{Linux Mint - Logo}
\end{figure}

Di seguito sono elencati i passaggi principali per l'installazione del sistema operativo:

\begin{enumerate}
    \item \textbf{Scaricare la distribuzione:} La distribuzione di Linux Mint 21.3 può essere scaricata direttamente dal sito ufficiale: \url{https://linuxmint.com/download.php}.
    \item \textbf{Creare una chiavetta USB avviabile:} Per scrivere l'immagine del sistema operativo su una chiavetta USB, è stato utilizzato un software come \textit{Rufus}. La chiavetta è stata preparata con una partizione di tipo GPT e con il file system NTFS per garantire la compatibilità.
    \item \textbf{Installazione in dual-boot:} L'installazione è stata effettuata in modalità \textit{dual-boot}, permettendo così di mantenere il sistema operativo preesistente insieme a Linux Mint. Il dual-boot è una configurazione che consente di scegliere quale sistema operativo avviare all'accensione del computer, garantendo la massima flessibilità nell'uso del dispositivo.
    \item \textbf{Modifica dell'ordine di boot:} Dopo aver creato la chiavetta USB, è stato necessario modificare l'ordine di avvio (boot) dal \textbf{BIOS/UEFI }del computer, per poi procedere con l'installazione definitiva di \textit{Linux Mint}.
\end{enumerate}


\section{Installazione ed avvio di ORB-SLAM 3}

In questo capitolo verrà descritta l'installazione e l'avvio del sistema ORB-SLAM3, uno dei più avanzati algoritmi di localizzazione e mappatura simultanea (SLAM). Verranno spiegati i passaggi principali necessari per configurare correttamente l'ambiente, installare le dipendenze e avviare ORB-SLAM3. L'obiettivo è fornire una guida dettagliata che permetta di eseguire l'algoritmo su un sistema basato su Linux, evidenziando eventuali problematiche comuni e come risolverle.

\subsection{Installazione del CUDA Toolkit}

Per poter sviluppare applicazioni con CUDA, è necessario installare il CUDA Toolkit, disponibile direttamente dal sito di NVIDIA. Questo toolkit contiene tutti gli strumenti necessari per la compilazione e l'esecuzione di codice parallelo su GPU, inclusi compilatori, librerie e campioni di codice. I passaggi principali necessari sono i seguenti e sono presi dal seguente tutorial \url{https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html}

\begin{itemize}
    \item 1. Controllare la versione di CUDA con il comando \textbf{nvidia-smi}
    \item 2. Scaricare la versione di CUDA corrispondente alla versione restituita dal comando precedente. Example: 
    \begin{verbatim}
    Driver Version: 535.171.04   CUDA Version: 12.2
    \end{verbatim}
    
    \item 3. Aprire il file .bashrc e inserire il seguente comando:
    \begin{verbatim}
    export PATH=/usr/local/cuda-{version}/bin/:$PATH
    \end{verbatim}
\end{itemize}

\subsection{Ottenimento del Kitty Data Set}

Il \textbf{KITTI dataset} è un insieme di dati ampiamente utilizzato nella ricerca su visione artificiale e guida autonoma. È composto da immagini, informazioni di navigazione (GPS/IMU), e dati LiDAR raccolti da veicoli in ambienti urbani, utilizzato per allenare e valutare algoritmi come rilevamento di oggetti, segmentazione semantica, e SLAM (Simultaneous Localization and Mapping).

Per installarlo è sufficiente loggarsi sul sito \url{https://www.cvlibs.net/datasets/kitti/eval_odometry.php} e scaricare il pacchetto \textit{odometry data set (grayscale, 22 GB)}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{img/kitti_dataset.png}
    \caption{Indicazione pacchetto da scaricare}
\end{figure}

\subsection{Installazione definitiva di ORB-SLAM3}

Per avvicinarci all'installazione finale di ORB-SLAM3 parallelo prima scarichiamo la versione non ottimizata, a seguire i passaggi descritti di seguito.

\begin{enumerate}
    \item \textbf{Clonare il repository:} Per prima cosa, clonare il repository di ORB-SLAM3 dal seguente URL: \url{https://github.com/UZ-SLAMLab/ORB_SLAM3}. Utilizzare il seguente comando nel terminale:

    \begin{verbatim}
    git clone https://github.com/UZ-SLAMLab/ORB_SLAM3.git
    \end{verbatim}

    \item \textbf{Eseguire la build:} Dopo aver clonato il repository, entrare nella directory di ORB-SLAM3 ed eseguire il processo di build con i comandi seguenti:

    \begin{verbatim}
    cd ORB_SLAM3
    chmod +x build.sh
    ./build.sh
    \end{verbatim}

    \item \textbf{Eseguire ORB-SLAM3:} Una volta completata la build, è possibile avviare ORB-SLAM3 con il comando seguente. Entrare nella directory principale di ORB-SLAM3 e utilizzare il comando:

    \noindent
    \begin{verbatim}
    
    ./Examples/Stereo/stereo_kitti Vocabulary/ORBvoc.txt 
    Examples/Stereo/KITTIX.yaml ~/Desktop/dataset/sequences/00
    \end{verbatim}

    Questo comando avvierà ORB-SLAM3 utilizzando il KITTI dataset per la modalità stereo.
    In base alla sequenza che andremo ad utilizzare bisogna cambiare \textbf{KITTIX.yaml} : 
    \begin{itemize}
        \item  KITTI00-02.yaml → sequenze 00,01,02.
        \item KITTY03.yaml → sequenze 03.
        \item KITTY04-12.yaml → sequenze 04,05,06,07,08,09,10,11,12.
    \end{itemize}
\end{enumerate}

\newline

Una volta che si è riusciti ad avviare la versione sopra citata, è possibile a passare alla versione parallela.
Infatti la versione attualmente in esecuzione, non è quella citata all'inizio del capitolo, questo perchè non contiene la parte di \textit{Estrazione ORB} parallelizata.
Per fare in modo di eseguire la versione in parallelo è necessario seguire i medesimi step con il seguente repository: \url{https://git.hipert.unimore.it/fmuzzini/cuda-accelerated-orb-slam}

\section{Analisi delle possibili ottimizazioni}

Durante l'analisi del codice di ORB-SLAM3, è stata individuata la funzione 
\begin{lstlisting}
void Frame::ComputeStereoMatches()}
\end{lstlisting} 
che si occupa dell'esecuzione della parte di \textbf{Stereo Matching}. 
Successivamente è stata eseguita una schematizzazione ed è stata analizzatata la sua struttura per identificare le possibili aree su cui intervenire per effettuare le ottimizzazioni.

\subsection{Schema generale}

Lo schema generale del funzionamento della funzione \texttt{ComputeStereoMatches()} è illustrato nell'immagine sottostante. Tale schema si compone di tre macro-funzioni principali e mostra il numero medio di iterazioni eseguite, permettendo di vedere il carico computazionale svolto.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{img/ComputeStereo_schema.png}
    \caption{Schematizzazione della funzione \textit{ComputeStereoMatches()} }
\end{figure}

\begin{enumerate}
    \item \textbf{Raccolta dei keypoints:} La prima funzione si occupa di selezionare, per ogni punto chiave(keypoint) dell'immagine destra, un intorno e di aggiungerlo a una struttura dati apposita, preparandolo per le fasi successive.
    \item \textbf{Selezione del miglior candidato:} La seconda funzione si concentra sulla ricerca del miglior candidato tra quelli selezionati nella fase precedente, valutando i vari intorni e scegliendo quello più adatto.
    \item \textbf{Filtraggio dei risultati:} La terza funzione applica filtri ai keypoints migliori, al fine di mantenere solo quelli che soddisfano determinati criteri di qualità, migliorando così la precisione complessiva del processo.
\end{enumerate}

\newpage
\subsection{Schema della ricerca dei migliori candidati}

Nell'illustrazione sottostante è possibile visionare la schematizzazione della funzione addetta a trovare i migliori punti chiave tra quelli candidati (cioè quelli con distanza minore dall'immagine a sinistra)

Ci sono due tipi di rappresentazioni. Quella ad \textbf{alta livello} che è astratta e non tiene conto delle strutture dati. Mentre la seconda si concentra sulle \textbf{strutture dati utilizzate} e \textbf{le iterazioni} eseguite per raggiungere il risultato atteso.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/find_distance_high_scheme.png}
    \caption{Schematizzazione ad alto livello }
\end{figure}

Come si può vedere nell'immagine questa parte dell'algoritmo calcolerà la varie distanze, tra i punti \textbf{candidati trovati} ed i punti effettivi nell'\textbf{immagine sinistra}.
E successivamente andrà a scartare i candidati non ottimali.



\begin{figure}[H]
    \centering
    \fbox{\includegraphics[width=1\textwidth]{img/find_best_distance.png}}
    \caption{Schematizzazione a basso livello}
\end{figure}

\subsection{Schema del filtraggio finale sui punti chiave}

In questa sezione, analizzeremo il processo di Sliding Window nel contesto del nostro sistema ORB-SLAM 3 nella versiona non ottimizata per CPU. 

\begin{enumerate}
    \item L'immagine seguente illustra schematicamente la componente \textbf{Sliding Window} del sistema:
    
    \item È importante notare che il filtraggio associato alla Sliding Window viene eseguito successivamente al calcolo della distanza minore di un singolo punto chiave. Questo permette, eventualmente, di non attendere il calcolo di tutte le distanze ma di eseguire il filtraggio ad ogni singolo punto chiave.
\end{enumerate}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{img/sliding_window_high.png}
    \caption{Schematizzazione a basso livello }
\end{figure}

\begin{figure}[h]
    \centering
    \fbox{\includegraphics[width=1\textwidth]{img/sliding_window.png}}
    \caption{Schematizzazione a basso livello }
\end{figure}





\chapter{ULAB}\label{ulab}

A seguito della panoramica delle librerie fatta in precedenza si è
deciso di procedere con l'approfondimento della libreria
ULAB, in quanto risulta avere una buona compatibilità con Numpy e Scipy,
due librerie fondamentali per il calcolo scientifico.\\
A seguito di una introduzione ci si concentrerà sullo studio della
compatibilità con le librerie sopracitate e con il testing delle
performance.

\section{Introduzione}\label{introduzione-1}

\textbf{ULAB} (MicroLab) è una libreria progettata per
l'ecosistema MicroPython, concepita per fornire
funzionalità simili a quelle offerte dalla popolare libreria
\textbf{NumPy} di Python, ma ottimizzate per
l'esecuzione su microcontrollori e dispositivi a risorse
limitate. ULAB consente agli sviluppatori di eseguire operazioni
matematiche e scientifiche avanzate direttamente su microcontrollori,
senza dover ricorrere a un hardware più potente o a linguaggi di
programmazione più complessi.

In contesti embedded, dove memoria e potenza di calcolo sono limitate,
le librerie standard di Python, come NumPy, non possono essere
utilizzate a causa della loro complessità e dei requisiti di risorse.
ULAB è stata sviluppata per colmare questa lacuna, offrendo un subset di
funzionalità simili a NumPy, ma con un footprint di memoria molto
ridotto e prestazioni ottimizzate per microcontrollori come ESP32,
STM32, e altri.

ULAB supporta una vasta gamma di operazioni, tra cui manipolazioni di
array, operazioni aritmetiche, funzioni matematiche avanzate, e
trasformate di Fourier. Queste funzionalità permettono agli sviluppatori
di eseguire calcoli direttamente sui dati acquisiti dai sensori o altre
fonti in tempo reale, rendendo possibile l'analisi e
l'elaborazione dei dati in applicazioni di Internet of
Things (IoT), robotica, automazione e molto altro.

Un aspetto cruciale di ULAB è la sua efficienza. Essendo progettata
specificamente per MicroPython, la libreria è estremamente leggera e
ottimizzata per eseguire operazioni matematiche con un overhead minimo.
Questo rende ULAB uno strumento indispensabile per coloro che sviluppano
applicazioni embedded che richiedono capacità di calcolo più avanzate
rispetto a quelle offerte nativamente da MicroPython, ma che non possono
permettersi l'uso di hardware più costoso o ingombrante.

In questa tesi, si esplorerà l'utilizzo di ULAB in
diversi scenari pratici, confrontando le sue prestazioni e capacità con
quelle delle librerie simili disponibili in Python standard. Verranno
presentati esempi applicativi per dimostrare come ULAB può essere
utilizzata per ottimizzare processi computazionali in ambienti embedded,
mantenendo un equilibrio tra efficienza e semplicità di sviluppo.

\section{Installazione di ULAB}\label{installazione-di-ulab}

ULAB, a differenza delle classiche librerie Python, non risulta essere
un modulo separato, ma permette di generare un nuovo firmware
MicroPython con la libreria ULAB integrata. ULAB offre infatti uno
script di build che è possible lanciare per
l'installazione.\\
Si illustrano i punti per la compilazione del firmware Unix, come
descritto nel README del progetto \cite{ulab_compiling}

\begin{enumerate}
\item
  clone del progetto
\end{enumerate}

\begin{verbatim}
    git clone https://github.com/v923z/micropython-ulab.git ulab
\end{verbatim}

\begin{enumerate}
\setcounter{enumi}{1}
\item
  entrare nella cartella del progetto
\end{enumerate}
\begin{verbatim}
    cd ulab
\end{verbatim}

\begin{enumerate}
\setcounter{enumi}{2}
\item
  compilazione del firmware\\
  \texttt{bash\ ./build.sh\ 2\ }\strut \\
  - il parametro \texttt{2} indica la dimensionalità massima degli
  ndarray, successivamente verrà analizzata più in dettaglio questa
  proprietà\\
  - il valore può variare da 1 a 4, dove 2 è il default
\item
  verrà creata la directory
  \texttt{./micropython/ports/unix/build-\{n\}/} in cui si troverà
  l'eseguibile \texttt{micropython-\{n\}}, dove
  \texttt{\{n\}} è il valore del parametro specificato precedentemente
\item
  nel caso si fosse eseguito il comando al punto 3 come indicato, a fine
  della compilazione sarà possibile avviare il firmware eseguendo il
  seguente comando
\begin{verbatim}
    ./micropython/ports/unix/build-2/micropython-2
\end{verbatim}
\end{enumerate}

Il firmware generato sarà un'eseguibile stand-alone con
dipendenze molto ridotte. Per Debian Bookworm basterà assicurarsi di
aver installato il pacchetto \texttt{binutils}

\section{Configurabilità di ULAB}

ULAB permette di configurazione il firmware in modo da adattarlo alle
caratteristiche della propria scheda e alle esigenze del programma che
dovrà girarci.\\
Per la configurazione ULAB utilizza l'header
\texttt{ulab.h} accessibile nella directory \texttt{code} del progetto
di ULAB. Nell'hein cui sono presenti\\
I parametri sono specificati nel file \texttt{ulab.h} come define.

\subsection{Dimensioni matrici}\label{dimensioni-matrici}

\texttt{ULAB\_MAX\_DIMS} indica la dimensionalità massima delle matrici
supportate dal firmware.\\
Il valore può andare da 1 a 4, di default è specificato il valore 2.\\
Questo è il parametro che più impatta la dimensione e la funzionalità
del firmware. È quindi buona norma settare un valore che rispetti le
esigenze del programma utilizzato. Utilizzare un firmware con
dimensionalità troppo alta potrebbe portare ad un aumento considerevole
della dimensione come si vedrà in seguito.

Questo parametro può essere anche specificato al momento della
compilazione del firmware per unix, come parametro del comando
\texttt{build.sh}, esempio:

\begin{verbatim}
    ./build.sh [matrix.dims]
\end{verbatim}

\subsection{Altri parametri}\label{altri-parametri}

\begin{itemize}
\item
  \texttt{NDARRAY\_HAS\_BINARY\_OPS} abilita la presenza degli operatori
  binari per gli ndarray, a costo di un aumento della dimensione del
  firmware

  \begin{itemize}
    \item
    valore di default: 0
  \end{itemize}
\item
  \texttt{ULAB\_HAS\_FUNCTION\_ITERATOR} indica di scrivere le
  iterazioni sulle dimensioni degli array come funzioni, invece che con
  macro, risparmiando spazio al costo di performance

  \begin{itemize}
    \item
    valore di default: 0
  \end{itemize}
\item
  \texttt{NDARRAY\_IS\_ITERABLE} abilita la funzione di iterazione degli
  ndarry, risparmiando spazio

  \begin{itemize}
    \item
    valore di default: 1
  \end{itemize}
\item
  \texttt{NDARRAY\_IS\_SLICEABLE} abilita lo slicing degli ndarray

  \begin{itemize}
    \item
    valore di default: 1
  \end{itemize}
\item
  \texttt{NDARRAY\_BINARY\_USES\_FUN\_POINTER} abilita
  l'utilizzo di puntatori a funzione nelle iterazoni
\item
  \texttt{ULAB\_HAS\_DTYPE\_OBJECT} determina se il dtype è un oggetto o
  un carattere, la rappresentazione con l'oggetto è più
  compatibile con Numpy, ma occupa più spazio.
\end{itemize}

\chapter{Performance testing}

Si è quindi deciso di testare vari versione del firmware analizzando lo
spazio occupato e le performance e confrontarli con python

\section{Ambiente di testing}

I testing sono stati fatti su ambiente docker con distribuzione Debian
Bookworm.\\

\section{Firmware testati}

Intanto si definiscono i due firmware di reference per performance e
dimensione:

\begin{enumerate}
\item
  \texttt{python3}: CPython 3.11 desktop, versione installata su Debian
  Bookworm
\item
  \texttt{ulab-2}: MicroPython versione 3.4\\
  1. tutti i parametri sono stati lasciati al valore di default\\
  2. questo è il firmware di reference per MicroPython\\
  Successivamente si è deciso di cambiare la dimensionalità massima
  delle matrici cambiando la macro \texttt{ULAB\_MAX\_DIMS}, ma
  lasciando tutti gli altri parametri invariati, generando i seguenti
  firmware:
\item
  \texttt{ulab-3}: firmware ULAB, con dimensionalità massima 2
\item
  \texttt{ulab-4}: firmware ULAB, con dimensionalità massima 4\\
  Successivamente, tenendo come riferimento la dimensionalità massima 3,
  si è deciso di generare i seguenti firmware per valutare
  l'impatto dei parametri sopracitati nelle performance
  e nello spazio ottenuto. Questi sono i firmware risultanti:
\end{enumerate}

\begin{itemize}
\item
  \texttt{ulab-2\_has\_binary\_ops\_0}: la macro
  NDARRAY\_HAS\_BINARY\_OPS è settata a 0
\item
  \texttt{ulab-2\_has\_function\_iterator\_1}: la macro
  ULAB\_HAS\_FUNCTION\_ITERATOR è settata a 1
\item
  \texttt{2\_ndarray\_is\_iterable\_0}: la macro NDARRAY\_IS\_ITERABLE è
  settata a 0
\item
  \texttt{ulab-2\_ndarray\_is\_sliceable\_0}: la macro
  NDARRAY\_IS\_SLICEABLE è settata a 0
\item
  \texttt{ulab-2\_ndarray\_uses\_fun\_pointer\_1}: la macro
  \texttt{NDARRAY\_BINARY\_USES\_FUN\_POINTER} è settata a 1
\item
  \texttt{ulab-2\_ulab\_has\_dtype\_object\_1}: la macro
  \texttt{ULAB\_HAS\_DTYPE\_OBJECT} è settata a 1
\end{itemize}

\section{Analisi dimensione firmware}


Si presenta un grafico di comparazione delle dimensioni dei diversi firmware:
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{size.png}
    \caption{Dimensioni firmware}
\end{figure}
Si presenta anche una versione tabellare dei dati graficati,
visualizzati in ordine decrescente di dimensione:

\begin{center}
\begin{tabular}{|c | c | c |}
\hline
firmware & size (Kb) & SLOC \\
\hline
python3 & 23616 & 6881 \\
ulab-4 & 6636 & 188787 \\
ulab-3 & 6504 & 180699 \\
ulab-2 & 6368 & 172502 \\
ulab-2\_has\_function\_iterator\_1 & 5640 & 178862 \\
ulab-2\_ulab\_has\_dtype\_object\_1 & 5552 & 173460 \\
ulab-2\_has\_binary\_ops\_0 & 5548 & 173212 \\
ulab-2\_ndarray\_is\_iterable\_0 & 5544 & 173158 \\
ulab-2\_ndarray\_is\_sliceable\_0 & 5520 & 172032 \\
ulab-2\_ndarray\_uses\_fun\_pointer\_1 & 5520 & 171419 \\
\hline
\end{tabular}
\end{center}

Dove con SLOC (Single Lines Of Code) si intendono le linee di codice Assembly del firmware.

Si può innanzitutto osservare che l'eseguibile Python
risulta di un ordine di grandezza più grande rispetto ai firmware
Micropython.\\
È chiaramente visibile come l'aumento della
dimensionalità delle aumenti lo spazio necesario, e come il cambiamento di
oguno dei valori di default delle macro porti ad una diminuzione della dimensione del firmware.

\subsection{Importanza delle SLOC}

È importante considerare la dimensione del firmware non solo perché un firmware più piccolo può essere caricato in una memoria limitata. Un firmware di dimensioni ridotte è anche più semplice da certificare, aspetto fondamentale in molte applicazioni critiche. La semplificazione della certificazione deriva dal fatto che un codice più piccolo è più facile da analizzare e verificare in tutte le sue parti.

In questo contesto, è stata prestata particolare attenzione anche al numero di istruzioni in assembly. In settori come la robotica biomedica, dove ogni componente del sistema deve essere estremamente affidabile, certificare l'intero firmware diventa essenziale. Avere un controllo preciso sul numero di istruzioni e sulla loro esecuzione aiuta a garantire che il sistema operi in modo sicuro e conforme alle normative.


\section{Descrizione test performance}\label{descrizione-test-performance}

I test eseguiti avevano l'obiettivo di utilizzare
diverse funzione di Numpy/ULAB.\\
I test sono stati eseguiti utilizzando un unico script Python
compatibile sia con CPython con librerie Numpy e Scipy, che con
MicroPython utilizzando con libreria ULAB.\\
Tutti i test sono ripetuti 100 volti per assicurarsi di eliminare
outliers\\
Lo script esegue 4 test che sono:

\begin{itemize}
\item
  matrix multiplication
\item
  interpolation
\item
  fft
\item
  linear system
\end{itemize}

\subsection{Matrix multiplication}\label{matrix-multiplication}

Si esegue una moltiplicazione tra due matrici 200 x 200, utilizzando la
funzione \texttt{np.dot()}

\subsection{Interpolation}\label{interpolation}

Interpolazione di due vettori di 100000 elementi, utilizzando la
funzione \texttt{np.interp()}

\subsection{FFT}\label{fft}

Calcolo della trasformata di Fourier di un vettore di 65536 elementi
utilizzando la funzione \texttt{np.fft.fft()}

\subsection{Linear system}\label{linear-system}

Risoluzione di un sistema lineare di equazioni composto da due matrici
di 800 elementi.\\
Questo test è l'unico dei 4 per cui è stato necessario
differenziare il codice per Numpy e ULAB, in quanto la funzione
\texttt{cho\_solve} di ULAB prende la matrice A, mentre per la stessa
funzione di Scipy è necessario prima fattorizzare A attraverso la
funzione \texttt{cho\_factor}

\section{Risultati test di performance}\label{risultati-test-di-performance}

Si mostrano i risultati dei test di performance per tutti i firmware: \\

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{performance.png}
    \caption{Test performance}
\end{figure}

qui si mostra, in ordine di velocità i firmware più performanti:


\begin{center}
\begin{tabular}{|c | c | c | c | c |}
\hline
firmware & min & max & median & std \\
\hline
python3 & 2.88771 & 839.135751 & 7.363561 & 117.306519 \\
ulab-2\_ndarray\_uses\_fun\_pointer\_1 & 6.69400 & 62.516000 & 12.849500
& 16.539355 \\
ulab-2\_has\_binary\_ops\_0 & 7.16300 & 56.928000 & 13.210500 &
14.668942 \\
ulab-2\_ulab\_has\_dtype\_object\_1 & 7.07300 & 52.117000 & 13.451000 &
14.362896 \\
ulab-2\_ndarray\_is\_iterable\_0 & 7.01100 & 52.777000 & 13.816000 &
14.325753 \\
ulab-4 & 6.33800 & 46.979000 & 14.359500 & 14.066975 \\
\textbf{ulab-2} & \textbf{6.46000} & \textbf{55.938000} &
\textbf{14.440000} & \textbf{15.599915} \\
ulab-2\_ndarray\_is\_sliceable\_0 & 6.95900 & 51.058000 & 14.914000 &
14.494034 \\
ulab-3 & 5.96900 & 57.900000 & 15.252500 & 14.811854 \\
ulab-2\_has\_function\_iterator\_1 & 6.30600 & 54.867000 & 15.292000 &
14.323220 \\
\hline
\end{tabular}
\end{center}

Salta subito all'occhio CPython, con performance
considerevolmente maggiori rispetto agli altri firmware.\\
Successivamente si può osservare che nessun firmware customizzato ha
peggiorato considerevolmente le performance rispetto a ulab-2 (il
firmware di reference), alcuni hanno anche migliorato le performance.

\subsection{Analisi performance Python nel test "linear-system"}

Inaspettatamente la performance di CPython nel test "linear system" è
considerevolmente peggiore rispetto agli altri firmware, rendendolo
l'unico test in cui le performance di CPython sono
inferiori a quelle di Micropython.\\
Analizzando più approfonditamente la questione il problema sembra essere
abbastanza evidente:


\begin{center}
\begin{tabular}{|c | c | c | c | c | c |}
\hline
firmware & min & max & median & std \\
\hline
python3 & 7.139038 & 839.135751 & 10.702393 &212.959248 \\
ulab-2 & 6.460000 & 8.041000 & 6.588000 & 0.281956 \\
\hline
\end{tabular}
\end{center}

Guardando questi valori si vede che il tempo massimo di CPython è molto
alto essendo più di 100 volte maggiore al valore massimo di ulab-2, il
valore minimo invece è abbastanza in linea con quello di ulab-2 con una
differenza di meno di un millisecondo. Questo ovviamente porta ad
un'altra deviazione standard.

Facendo riferimento a quanto presentato prima questo test è
l'unico per cui il codice è diverso tra MicroPython e
CPython, a causa della differenza di implementazione dei due.

Isolando il test si può infatti osservare come la maggior parte del
tempo sia preso nella fattorizzazione di cholesky, mentre il tempo di
risoluzione è molto basso.\\
Qui si mostra un esempio dei tempi di esecuzione di entrambe le parti:

\begin{center}
\begin{tabular}{|c | c | c |}
\hline
 &fattorizzazione & risoluzione \\
\hline
tempo (ms) & 99.856444 & 2.309838 \\
\hline
\end{tabular}
\end{center}

I tempi però si abbassano considerevolmente nelle iterazioni
successive.\\
Come si può vedere da i tempi di esecuzione delle varie iterazioni si ha
una considerevole riduzione dei tempi di esecuzione a seguito della nona
iterazione, che viene mantenuta fino alla fine:

\begin{center}
\begin{tabular}{|c | c | c |}
\hline
iterazione & tempo (ms) \\
\hline
1 & 839.135751 \\
2 & 392.83769 \\
3 & 584.091132 \\
4 & 736.04277 \\
5 & 788.097906 \\
6 & 692.523665 \\
7 & 583.249705 \\
8 & 555.962178 \\
9 & 440.824691 \\
10 & 10.586065 \\
11 & 9.417711 \\
12 & 7.88812 \\
... & ... \\
\hline
99 & 34.799827 \\
\hline
\end{tabular}
\end{center}

Si sospetta che questo fenomeno sia dovuto da un caching del risultato

\subsubsection{Differenze di implementazione}

Come anticipato il test "linear system" mostra la non completa compatibilità tra la libreria ULAB e la libreria Scipy.
Analizzando il codice del test si possono notare le differnze:

\begin{verbatim}
if using_ulab:
    res = scipy.linalg.cho_solve(a, b)
else:  
    factor = scipy.linalg.cho_factor(a)
    res = scipy.linalg.cho_solve(factor, b)
\end{verbatim}

Per l'esecuzione dello stesso script su entrambi i tipi di firmware è stato necessario differenziare la chiamata della funzione 
\texttt{cho\_solve}. Facendo riferimento alla documentazione di ULAB sulla funzione \texttt{cho\_solve} \cite{ulab_cholesky} si 
può leggere questa frase: \textit{"As opposed to scipy, the function simply takes the Cholesky-factorised matrix"} esplicitando appunto che la funzione non è direttamente compatibile con la corrispondente funzione della libreria Scipy.\\

Analizzando più in dettaglio i tempi di esecuzione, si che la maggior parte del tempo di esecuzione delle prime iterazioni è impiegato nella fattorizzazione della matrice, ovvero il passaggio non necessario per MicroPython.

Questo è un buon esempio dell'accortezza che è necessario utilizzare per eseguire il porting di uno script da Python a MicroPython.


\subsection{Nota sui numeri complessi}

Dai risultati del test FFT si è osservato che i valori restituiti da MicroPython non risultano diversi in struttura rispetto ai valori di riferimento restituiti da Python.
In particolare si osserva che Python restituisce un array unico con il risultato, mentre Micropython restituisce un array contente due array differenti.\\
La motivazione è che ULAB di base non supporta i numeri complessi, ma invece ritorna il risultato creando un array per la parte reale ed un array per la parte complessa.\\
Seguendo le indicazioni presentate nella documentazione della funzione fft, si vede che è però possibile attivare questa funzionalità a costo dell'utilizzo del 50\% in più di RAM.

Per attivare la gestione dei numeri complessi è sufficinete assicurarsi che la macro \texttt{ULAB\_SUPPORTS\_COMPLEX} sia settata a 1 e assicurarsi di settare ad 1 anche la macro \texttt{ULAB\_FFT\_IS\_NUMPY\_COMPATIBLE}

\subsection{Errori di approssimazione}

Confrontando i valori ritornati dai test fatti si nota come in due test i valori siano considerevolmente differenti rispetto ai valori di riferimento generati da Python. Si parte ad analizzare i valori risultanti dal test linear system, qui si possono vedere i primi 3 elementi di ciascun firmware:

\begin{verbatim}
Python: [0.0, 0.006257822277847307,
            0.008343763037129748]
ULAB 2: [0.0, 0.003128911138923654,
            0.002781254345709915]
\end{verbatim}

È subito evidente una considerevole differenza, al punto che persino la cifra significativa risulta differente.
Si nota un comportamento simile anche per il test fft:

\begin{verbatim}
Python: [(-1.4026391268080751e-14+0j),
            (1.570784341276565-32767.74994789285j),
            (-6.391749161391903e-05+0.6666836188068311j)]
ULAB 2: [(5.122156919054285e-08+0j),
            (1.570784369533921-32767.7499478676j),
            (-6.38901903868286e-05+0.666683635324262j)]
\end{verbatim}

Qui l'impatto sembra più ridotto, visto che per alcuni valori la corrispondenza è esatta, ma l'alcuni errori sono comunque presenti. \\
Per gli altri test invece non si osservano errori di approssimazione. Investigando meglio sulla problematica si ipotizza che il problema sia dovuto ad alcuni errori di approssimazione su MicroPython, che, anche se inizialmente molto piccoli, in base al tipo di operazione possono diventare sostanziali.

L'ipotesi si è verificata con la semplice radice di 2. Questo è il risultato di Pyhton:

\begin{verbatim}
1.41421356237309514547
\end{verbatim}

mentre questo è il risultato di Micropython:

\begin{verbatim}
1.41421356237309510107
\end{verbatim}

Dalla diciassettesima cifra decimale si nota infatti che i due valori divergono, introducendo quindi un errore nei calcoli. 
Si ipotizza che il motivo di questa differenza sia dato dal fatto che MicroPython, essendo più ottimizzato per lo spazio, sia capace di meno precisione rispetto a Python.

\chapter{Conclusioni}

In questa tesi, si è esplorato e confrontato l'uso di Python e MicroPython, con particolare attenzione alle applicazioni scientifiche e ai dispositivi a risorse limitate. Si è evidenziato come, sebbene Python sia un linguaggio versatile e potente, esso non risulti ottimizzato per ambienti con risorse ridotte, mentre MicroPython rappresenta una soluzione più efficiente e mirata in tali contesti.

Le principali differenze riscontrate riguardano le dimensioni dell'interprete, la gestione della memoria e la disponibilità delle librerie. MicroPython, grazie alla sua leggerezza e alla capacità di essere adattato all'hardware su cui viene eseguito, riesce a fornire molte delle funzionalità di Python, sacrificando alcune caratteristiche avanzate per garantire un uso ottimale delle risorse.

L'analisi comparativa delle prestazioni ha mostrato che MicroPython, in combinazione con librerie come ULAB, può essere una valida alternativa per operazioni scientifiche di base su sistemi embedded. I test eseguiti hanno dimostrato che, sebbene Python mantenga prestazioni superiori su piattaforme con risorse adeguate, MicroPython è in grado di competere efficacemente in ambienti a risorse limitate, specialmente quando si applicano ottimizzazioni specifiche.

Tuttavia, come emerso dai test relativi ai sistemi lineari e alle operazioni FFT, MicroPython presenta limitazioni in termini di precisione numerica rispetto a Python. Questo aspetto è rilevante nelle applicazioni in cui l'accuratezza dei calcoli è essenziale, come nella robotica o nel controllo industriale. Il compromesso tra precisione e prestazioni diventa, quindi, un fattore cruciale nella scelta della piattaforma di sviluppo per applicazioni embedded.

Un altro aspetto emerso riguarda la possibilità di personalizzazione del firmware. La configurabilità del firmware ULAB consente di adattarlo alle esigenze specifiche del dispositivo, offrendo flessibilità, ma richiede una comprensione approfondita delle opzioni di configurazione e delle esigenze del progetto. Come mostrato dai test, le variazioni nei parametri di compilazione possono influenzare significativamente sia le prestazioni sia le dimensioni del firmware, permettendo di bilanciare efficienza e capacità computazionale.

In conclusione, MicroPython offre un'ottima alternativa per coloro che necessitano di soluzioni Python-like in ambienti embedded con risorse limitate. Nonostante le limitazioni rispetto a Python standard, MicroPython, in combinazione con strumenti come ULAB, fornisce una base solida per lo sviluppo di applicazioni scientifiche e tecniche su microcontrollori. È però essenziale valutare attentamente le esigenze del progetto, considerando i compromessi in termini di precisione e prestazioni, al fine di scegliere la piattaforma più adatta. Questo secondo \cite{CUDAtutorial}


\bibliographystyle{apalike}
\bibliography{bibliografia}  % type -> @online,@misc,@unpublished,@thesis,@article,@book,...


\end{document}


